%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inteligencia Artificial}
\label{sec:ml}

La inteligencia artificial (\gls{ia}) se constituye como la disciplina de diseño y creación de sistemas de software o hardware de imitación de la inteligencia humana para la realización de tareas. Generalmente, estos sistemas obtienen la capacidad de aprendizaje, razonamiento, planificación y toma de decisiones a partir del entrenamiento basado en grandes cantidades de información. En otros términos, adquieren e interpretan datos del entorno o de un contexto específico y, teniendo en cuenta un objetivo determinado, procesan la información contenida para decidir la siguiente acción a realizar. Adicionalmente, son capaces de adaptar su comportamiento ante el análisis o detección de cambios en el entorno. \cite{iagov} \cite{iaazure}

\vspace{3mm}

La \gls{ia} se postula como una de las tecnologías más revolucionarias y transformadoras de la actualidad y del futuro cercano por su potencial de aplicación en numerosos ámbitos. No obstante, el origen del término se remonta al año 1956, cuando el científico John McCarthy introdujo la idea de crear máquinas inteligentes tomando como base las teorías de computación de los matemáticos Norbert Wiener y John von Neumann en los años 40.~\cite{iagov}

\vspace{3mm}

En la última década, el impulso de la \gls{ia} y los grandes avances que se han producido en este campo vienen dados principalmente por el aumento de las capacidades de los equipos informáticos y el acceso a grandes cantidades de recursos computacionales en las herramientas especializadas e infraestructuras de nube. En otros términos, se posibilita un almacenamiento masivo de datos y, en consecuencia, mejoras significativas en el desarrollo de algoritmos y técnicas, que cada vez se vuelven más complejas y precisas. Además, cabe destacar la contribución que han supuesto las tecnologías \gls{iot} (ver Sección \ref{sec:iot}) en la generación de grandes volúmenes de datos y, por tanto, en el avance de la \gls{ia} al proporcionar información de valor con la que entrenar estos algoritmos.

\vspace{3mm}

Entrando en estas técnicas, se establecen dos divisiones dentro del campo de la \gls{ia} en función de la naturaleza del aprendizaje: el automático (\acrfull{ml}) y el profundo (\acrfull{dl}). En las siguientes Secciones (ver Secciones \ref{sec:ml} y \ref{sec:dl}) se expondrán las características que presentan cada una de estas ramas, además de entrar en profundidad en el funcionamiento de los modelos que se emplearán en el desarrollo de este \gls{tfm}.

\subsection{Machine Learning (\acrshort{ml})}
\label{sec:ml}

La rama de aprendizaje automático (\acrfull{ml}) se enfoca en el desarrollo de modelos que sean capaces de aprender y tomar decisiones en base a la introducción de datos. Durante el proceso se realizan observaciones a la información existente y, en función de la técnica a emplear, se aplican algoritmos estadísticos para identificar patrones en los datos. Por lo tanto, mediante un entrenamiento progresivo en el tiempo, un modelo de \gls{ml} puede llegar a realizar predicciones sobre datos futuros sin la intervención humana y con una alta precisión. Como se representa en la Figura \ref{fig:ml}, las técnicas de \gls{ml} pueden pertenecer a tres categorías diferentes en función del enfoque de la información y de los objetivos que se pretenden conseguir con su aplicación: \cite{mlcat} \cite{iageeks} \cite{mltlf}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/teoria/ml.jpeg}
    \caption{Categorías de \acrshort{ml} \cite{metal}}
    \label{fig:ml}
\end{figure}

\begin{itemize}
    \item Aprendizaje no supervisado: Se parte de datos sin etiquetar para el entrenamiento de los modelos. Es decir, se parte del conocimiento de unos datos de entrada, pero no de la salida, por lo que el objetivo de los modelos no supervisados se basa en explorar posibles patrones en los datos mediante \textit{clustering}.
    \item Aprendizaje supervisado: Se manejan conjuntos de datos con un conocimiento de las posibles salidas, las cuales se proporcionan en forma de etiquetas o de valores numéricos. Las técnicas de aprendizaje supervisado buscan una función óptima que consiga, dadas unas determinadas variables o características de entrada, predecir la salida con cierta precisión y son aplicables a dos tipos de problemas: de regresión, si a partir de las variables de entrada se desea predecir un valor numérico continuo a la salida, o de clasificación, si se predefinen diferentes clases y se asignan las variables de entrada a una de ellas. 
    \item Aprendizaje por refuerzo: Las técnicas englobadas en este tipo aprenden por prueba y error, por lo que su entrenamiento a lo largo del tiempo y la experiencia permiten optimizar los resultados sin la necesidad de disponer de un gran volumen de datos.
\end{itemize}

Una vez expuestas las diferentes categorías donde se engloban las técnicas de \gls{ml}, es preciso indicar que se va a centrar el estudio en el \textbf{aprendizaje supervisado} y, en particular, en la resolución de problemas de \textbf{clasificación binaria}. El motivo principal viene dado porque el objetivo que se persigue con la realización de este \gls{tfm} se basa en el desarrollo de modelos que permitan detectar y predecir posibles errores que se pueden producir en una \gls{sg} durante el proceso de distribución energética. 

\vspace{3mm}

Como se detallará más adelante en la Sección \ref{sec:cambiosden2ne}, se creará un conjunto de datos etiquetados en función de la existencia de error o no para entrenar los modelos. Teniendo en cuenta esto, a continuación, se incluyen dos Secciones (ver Secciones \ref{sec:mlsvm} y \ref{sec:mlrf}) para presentar el funcionamiento de las dos técnicas de \gls{ml} que se desarrollarán posteriormente.

\subsubsection{Random Forest (\acrshort{rf})}
\label{sec:mlrf}

El modelo de Bosques Aleatorios (del inglés \acrfull{rf}) está fundamentado en la aplicación de un conjunto de árboles de decisión. Cada uno de estos árboles se definen como estimadores y crecen mediante un proceso de entrenamiento sobre un subconjunto de datos extraído de forma aleatoria del conjunto completo. A la salida del esquema de estimadores se obtiene una predicción final basada en la votación mayoritaria de los árboles. No obstante, es un método que se puede emplear también en problemas de regresión y, en este caso, se proporcionaría un valor numérico promedio, calculado a partir de todas las salidas. En la Figura \ref{fig:rf} se representa de forma gráfica el diagrama de flujo que expone el funcionamiento del modelo. \cite{rfmedium}

\vspace{3mm}

El proceso de construcción de los árboles de decisión se constituye por operaciones de división que van formando nodos y ramificaciones de forma iterativa. El objetivo es lograr la mayor ganancia de información posible a través de la valoración de la importancia que presentan cada una de las características del conjunto de datos. Es decir, en cada nodo \textit{m} del árbol se cuantifica la ganancia de información que presenta cada característica y, después, se selecciona la que tenga el mayor valor (\textit{j}). Se aplica una división de los datos existentes en dos subconjuntos homogéneos de menor tamaño: \cite{rfmedium2}

\begin{equation}
    \begin{aligned}
        \theta &= (j, t_m) \\
        Q_m^{\text{left}}(\theta) &= \{(x, y) \mid x_j \leq t_m\} \\
        Q_m^{\text{right}}(\theta) &= Q_m \setminus Q_m^{\text{left}}(\theta)
    \end{aligned}
\end{equation}
    

Donde:
\begin{itemize}
    \renewcommand{\labelitemi}{}
    \item \( \theta \) es la operación de división en un nodo \textit{m}.
    \item \( j\) es la característica seleccionada.
    \item \( t_m\) es el nivel de partición de los datos.
    \item \( Q_m^{left}\) es el subconjunto de datos en la ramificación izquierda.
    \item \( Q_m^{right}\) es el subconjunto de datos en la ramificación derecha.
\end{itemize}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{img/teoria/rf.png}
    \caption{Modelo \acrshort{rf} \cite{rfmedium}}
    \label{fig:rf}
\end{figure}

\vspace{3mm}

Este proceso se va repitiendo en todos los nodos hasta llegar al final del árbol y, en cuanto al cálculo de la ganancia de información, es preciso introducir el concepto de impureza. Este se define como el procedimiento de evaluación de la calidad de las divisiones que se producen en los nodos. En otros términos, se puede expresar que indica el nivel de ruido que hay en los datos de los subconjuntos creados mediante una función de impureza \textit{H()}:

\vspace{3mm}

\begin{equation}
    \begin{aligned}
        G(Q_m, \theta) = \frac{n_m^{left}}{n_m} H(Q_m^{left}(\theta)) + \frac{n_m^{right}}{n_m} H(Q_m^{right}(\theta))
    \end{aligned}
\end{equation}

\vspace{3mm}

Tomando esto en consideración, se pueden emplear dos funciones o métodos de medición de la impureza:~\cite{rfmedium2} \cite{scikitrf}

\begin{itemize}
    \item Entropía: Su objetivo está orientado a establecer divisiones de los datos de forma que la entropía en los nodos inferiores o hijos sea menor que la del nodo superior o padre. Para ello, se aplica la teoría de la entropía de Shannon \cite{rfmedium2} para escoger las características que minimicen la entropía y, en consecuencia, la incertidumbre y la función de pérdidas. De forma contraria, en un nodo se obtiene una entropía máxima cuando las clases están representadas homogéneamente en el conjunto de datos.
    
    \begin{equation}
        \begin{aligned}
            H(Q_m) = - \sum_k p_{mk} \log(p_{mk})
        \end{aligned}
    \end{equation}
    
    \item Gini: La impureza es definida a partir del cálculo de la probabilidad de una característica determinada que es clasificada erróneamente cuando se selecciona aleatoriamente. Un índice Gini nulo expresa una clasificación pura donde todos los datos corresponden a una clase específica, en cambio, un índice Gini igual a 1, representa una distribución aleatoria de los datos.
    
    \begin{equation}
        \begin{aligned}
            H(Q_m) = \sum_k p_{mk} (1 - p_{mk})
        \end{aligned}
    \end{equation}
\end{itemize}

Donde:
\begin{itemize}
    \renewcommand{\labelitemi}{}
    \item \(p_{mk}\) es la proporción de datos de entrenamiento que pertenecen a una clase determinada.
    \item \(H(Q_m)\) es la función de impureza.
\end{itemize}


\subsubsection{Support Vector Machines (\acrshort{svm})}
\label{sec:mlsvm}

El modelo de Máquinas de Vector Soporte (del inglés \acrfull{svm}) es una técnica que se emplea tanto en problemas de regresión como de clasificación. Se basa en el concepto del \textit{Maximal Margin Classifier} o \textit{Hard Margin Classifier}, puesto que tiene el fin principal de encontrar el hiperplano óptimo que consiga una máxima separación entre dos clases diferentes dentro de un espacio de características. Esta separación se define como margen y se mide como la distancia perpendicular que existe desde el hiperplano hacia los vectores de soporte, que son las muestras más cercanas a la frontera de separación de clases. En la Figura \ref{fig:svm} se representa el cálculo del mejor hiperplano entre dos clases de datos. \cite{svmmedium2} \cite{svmciencia}

\vspace{3mm}

Por ello, se puede expresar que los vectores de soporte son los puntos más críticos del plano para asignar una de las clases predefinidas a los nuevos vectores de datos a la entrada. Este proceso de clasificación viene dado por la siguiente expresión: \cite{svmmedium} 

\begin{equation}
    \begin{aligned}
        y_i(\mathbf{w} \cdot \mathbf{x_i} + b) \geq M
    \end{aligned}
\end{equation} 

    Donde:
\begin{itemize}
    \renewcommand{\labelitemi}{}
    \item \(y_i\) es la etiqueta de la clase. 
    \item \(x_i\) es el vector de características.
    \item \(w\) es el vector de pesos o coeficientes del hiperplano.
    \item \(b\) es el sesgo ajustado en el modelo.
    \item \(M\) es el ancho definido para el margen.
\end{itemize}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\textwidth]{img/teoria/svm.png}
    \caption{Cuantificación del hiperplano óptimo entre dos clases en el \acrshort{svm} \cite{svmmedium2}}
    \label{fig:svm}
\end{figure}

\vspace{3mm}

No obstante, en la práctica es complicado determinar un hiperplano si no se trata de un problema ideal y basado en datos linearmente separables. Generalmente, en los casos reales si se intenta forzar un máximo margen, se pueden llegar a producir problemas de sobreentrenamiento, ya que nuevos datos pueden suponer grandes variaciones en el hiperplano. En la Figura \ref{fig:svmerror} se representa la opción alternativa que se lleva a cabo en la mayoría de casos, basada en obtener un margen máximo, permitiendo cierto grado de error (\textit{Soft Margin Classifier}). \cite{matlab} \cite{svmciencia}

\vspace{3mm}

Esto tiene como consecuencia que existan una serie de vectores clasificados de forma errónea en el plano. En este proceso se introduce el concepto de variables de holgura (\textit{slack variables}) a partir de la siguiente expresión:

\begin{equation}
    \begin{aligned}
        \xi_i = \max(0, 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b))
    \end{aligned}
\end{equation} 

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{img/teoria/svm2.png}
    \caption{Representación de vectores clasificados erróneamente dentro de los márgenes establecidos en el \acrshort{svm} \cite{svmmedium}}
    \label{fig:svmerror}
\end{figure}

\vspace{3mm}

Estas variables determinan si un vector está correctamente clasificado o si se encuentra en una zona errónea del margen o del hiperplano, respectivamente. En el caso de posición incorrecta, el valor de la variable define la distancia al margen o el error cometido:

\begin{equation}
    \begin{aligned}
        \xi_i = 0 & \text{ si } 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \leq 0 \\
        0 \text{<} \xi_i \text{<} 1 & \text{ si } 0 < 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \leq 1 \\
        \xi_i \text{>} 1 & \text{ si } 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b) > 1 \\
    \end{aligned}
\end{equation} 

\vspace{3mm}

El sumatorio de todas las variables de holgura que existen en el plano vienen incluidas en el objetivo de la función de pérdida del modelo para que el error de clasificación sea mínimo, a la vez que el margen es máximo. Como es preciso encontrar un cierto equilibrio entre ambos, el \gls{svm} se determina como una técnica de optimización convexa que se fundamenta en el hiperparámetro \textit{C}. En la Figura \ref{fig:parametroc} se representa de forma gráfica cómo su valor establece el control de forma inversamente proporcional de la cantidad de muestras clasificadas erróneamente, por lo que se define como el parámetro de regularización del modelo. Es decir, cuanto más alto sea su valor, menores violaciones del margen y del hiperplano serán permitidas y más se enfocará el modelo en un \textit{Maximal Margin Classifier}.~\cite{svmciencia}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/teoria/parametroc.png}
    \caption{Configuración del parámetro de regularización \textit{C} \cite{velocity}}
    \label{fig:parametroc}
\end{figure}

\vspace{3mm}

Además de permitir cierto grado de error como se ha expuesto anteriormente, para enfrentar problemas con datos no linearmente separables, es imprescindible incrementar las dimensiones del espacio original de características. En este caso se introduce el concepto de kernel como función para obtener un nuevo espacio dimensional diferente al original donde exista una mayor probabilidad de que los datos sí sean linearmente separables. La aplicación de los diferentes tipos de kernels que permiten las \gls{svm} se caracteriza a partir de las expresiones expuestas a continuación:~\cite{svmciencia}~\cite{velocity}

\begin{equation}
    \begin{aligned}
        \text{Kernel lineal: } K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j \\
        \text{Kernel polinómico: } K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i \cdot \mathbf{x}_j + r)^d \\
        \text{Kernel radial o gaussiano: } K(\mathbf{x}_i, \mathbf{x}_j) = \exp \left( -\gamma \| \mathbf{x}_i - \mathbf{x}_j \|^2 \right) \\
        \text{Kernel sigmoid: } K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i \cdot \mathbf{x}_j + r)
    \end{aligned}
\end{equation}


\vspace{3mm}

Para obtener una mayor compresión, se representa de forma adicional la Figura \ref{fig:rbf}. En la misma se puede apreciar gráficamente cómo se produce la transformación dimensional de las características, especificando para el caso del kernel gaussiano o \gls{rbf}.

\vspace{3mm}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/teoria/rbf.png}
    \caption{Aplicación del kernel \acrshort{rbf} \cite{rbf}}
    \label{fig:rbf}
\end{figure}

\subsection{Deep Learning (\acrshort{dl})}
\label{sec:dl}

El campo del aprendizaje profundo (\acrfull{dl}) se enfoca principalmente en la creación de redes neuronales que imiten el comportamiento y la estructura lógica que tiene el cerebro humano para buscar patrones en los datos. A diferencia de las técnicas de \gls{ml}, el \gls{dl} se vuelve más eficiente en el análisis de grandes volúmenes de datos, puesto que el \gls{ml} presenta ciertas limitaciones en este aspecto. \cite{metal} 

\vspace{3mm}

De la misma forma, el \gls{dl} presenta un mejor funcionamiento en la identificación de patrones cuando se manejan características complejas, aportando resultados con una mayor precisión que el \gls{ml}. Es por ello que las técnicas de \gls{dl} cobran una gran importancia en entornos donde los datos no son estructurados, como se produce en el caso de las imágenes, texto y audio. En este caso se pueden introducir en aplicaciones de reconocimiento de voz, procesado de lenguaje natural (del inglés \gls{nlp}) o visión artificial para la detección de objetos y clasificación de imágenes. \cite{iageeks}

\vspace{3mm}

Por otro lado, a nivel estructural las diferencias entre las técnicas de \gls{ml} y las de \gls{dl} radican principalmente en el tratamiento de las características o variables de los datos. En el caso de emplear \gls{ml}, si se manejan conjuntos con un gran número de características, sería necesario introducir un paso previo de diseño y selección de las más relevantes. Esto, como se puede apreciar en la Figura \ref{fig:features}, no ocurre en el desarrollo de un modelo de \gls{dl}, ya que el tratamiento se produce internamente dentro de la red reuronal. \cite{valohai}

\vspace{3mm}

Una vez presentada la rama de aprendizaje profundo, se introduce una Sección dedicada a la exposición del modelo de \gls{dl} que se desarrollará para lograr los objetivos de este \gls{tfm} (ver Sección \ref{sec:dlann}). 

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{img/teoria/mlvsdl.png}
    \caption{Diferencias estructurales entre el \acrshort{ml} y el \acrshort{dl} \cite{valohai}}
    \label{fig:features}
\end{figure}

\subsubsection{Red Neuronal Artificial (\acrshort{ann}) y Perceptrón Multicapa (\acrshort{mlp})}
\label{sec:dlann}

Las redes neuronales artificiales (\gls{ann}) y, en particular, los perceptrones multicapa (del inglés \gls{mlp}), se construyen a partir de distintas capas de nodos y conexiones que replican la estructura neuronal que tiene el cerebro humano. Como se representa en la Figura \ref{fig:ann}, se define una capa de entrada (\textit{input layer}), una o múltiples capas ocultas (\textit{hidden layer}) y una capa de salida (\textit{output layer}). El entrenamiento de la arquitectura de neuronas viene dado por las conexiones que se van estableciendo entre los nodos de las capas y los pesos y los valores de umbral que se van especificando durante el proceso. \cite{ibmann}

\vspace{3mm}

En otros términos, se toma cada nodo como una aplicación del modelo de regresión lineal, donde se parte de unos datos en la capa de entrada, que son ponderados con unos determinados pesos en cada capa oculta de la siguiente forma:

\begin{equation}
    \begin{aligned}
        z = \mathbf{w}^T \cdot \mathbf{x} + b
    \end{aligned}
\end{equation} 

Donde:
\begin{itemize}
    \renewcommand{\labelitemi}{}
    \item \(z\) es la salida de cada capa oculta. 
    \item \(w\) es el vector de pesos. 
    \item \(b\) es el sesgo.
    \item \(x\) es el vector de entrada
\end{itemize}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{img/teoria/ann.jpg}
    \caption{Arquitectura de un \acrshort{mlp} \cite{ann}}
    \label{fig:ann}
\end{figure}

\vspace{3mm}

De la misma forma, en cada capa oculta se aplica una función de activación o de umbral, que impone un valor límite a la salida final del nodo en cuestión y, en consecuencia, a la entrada del nodo siguiente al que se encuentre conectado. Las funciones de activación principales que se pueden emplear se representan en la Figura \ref{fig:functions} y vienen dadas por las siguientes expresiones: \cite{factiv} \cite{functions}

\begin{equation}
    \begin{aligned}
        \text{Función Sigmoidal (Logística):} \quad f(x) = \frac{1}{1 + e^{-x}} \\
        \text{Función ReLU (Rectified Linear Unit):} \quad f(x) = \max(0, x) \\
        \text{Función tanh (Tangente Hiperbólica):} \quad f(x) = \tanh(x) \\
        \text{Función Softmax:} \quad \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \\
    \end{aligned}
\end{equation}

Teniendo todo el procedimiento anterior en cuenta, se puede expresar que, a partir de una serie de características, el objetivo es escoger aquellas que contribuyan a obtener una mayor precisión en la clasificación de datos de entrada futuros, puesto que se van estableciendo las conexiones óptimas entre los nodos de las capas ocultas con un entrenamiento progresivo. Es preciso indicar que el funcionamiento del modelo se basa en una red de propagación hacia delante, lo que supone que se defina un flujo unidireccional, desde la entrada a la salida. 

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{img/teoria/functions.png}
    \caption{Funciones de activación \cite{functions}}
    \label{fig:functions}
\end{figure}

\vspace{3mm}

No obstante, para ajustar las ponderaciones se aporta una retroalimentación, que depende directamente de la función de coste y de la salida del modelo en cada ejecución o \textit{epoch}. Principalmente, se pueden utilizar dos métodos para minimizar la función de coste: el algoritmo adaptativo \textit{adam} (del inglés \textit{Adaptive Moment Estimation}), que presenta una tasa de aprendizaje adaptativa para cada parámetro, y el gradiente descendiente (del inglés \gls{sgd}), que actualiza los pesos de la red en función del gradiente de la función de pérdida con respecto a los pesos. \cite{ibmann}

