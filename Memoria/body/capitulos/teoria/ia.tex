%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inteligencia Artificial}
\label{sec:ml}

La inteligencia artificial (\gls{ia}) se constituye como la disciplina de diseño y creación de sistemas de software o hardware de imitación de la inteligencia humana para la realización de tareas. Generalmente, estos sistemas obtienen la capacidad de aprendizaje, razonamiento, planificación y toma de decisiones a partir del entrenamiento basado en grandes cantidades de información. En otros términos, adquieren e interpretan datos del entorno o de un contexto específico y, teniendo en cuenta un objetivo determinado, procesan la información contenida para decidir la siguiente acción a realizar. Adicionalmente, son capaces de adaptar su comportamiento ante el análisis o detección de cambios en el entorno. \cite{iagov} \cite{iaazure}

\vspace{3mm}

La \gls{ia} se determina como una de las tecnologías más revolucionarias y transformadoras de la actualidad y del futuro cercano por su potencial de aplicación en numerosos ámbitos. No obstante, el origen del término se remonta al año 1956, cuando el científico John McCarthy introdujo la idea de crear máquinas inteligentes tomando como base las teorías de computación de los matemáticos Norbert Wiener y John von Neumann en los años 40.~\cite{iagov}

\vspace{3mm}

En la última década, el impulso de la \gls{ia} y los grandes avances que se han producido en este campo vienen dados principalmente, por el aumento de las capacidades de los equipos informáticos y el acceso a grandes cantidades de recursos computacionales en las herramientas especializadas e infraestructuras de nube. En otros términos, se posibilita un almacenamiento masivo de datos y, en consecuencia, mejoras significativas en el desarrollo de algoritmos y técnicas, que cada vez se vuelven más complejas y precisas. 

\vspace{3mm}

Entrando en estas técnicas, se establecen dos divisiones dentro del campo de la \gls{ia} en función de la naturaleza del aprendizaje: el automático (\acrfull{ml}) y el profundo (\acrfull{dl}). En las siguientes Secciones (ver Secciones \ref{sec:ml} y \ref{sec:dl}) se expondrán las características que presentan cada una de estas ramas, además de entrar en profundidad en el funcionamiento de los modelos que se emplearán en el desarrollo de este \gls{tfm}.

\subsection{Machine Learning (\acrshort{ml})}
\label{sec:ml}

La rama de aprendizaje automático (\acrfull{ml}) se enfoca en el desarrollo de modelos que sean capaces de aprender y tomar decisiones en base a la introducción de datos. Durante el proceso se realizan observaciones a la información existente y, en función de la técnica a emplear, se aplican algoritmos estadísticos para identificar patrones en los datos. Por lo tanto, mediante un entrenamiento progresivo en el tiempo, un modelo de \gls{ml} puede llegar a realizar predicciones sobre datos futuros sin la intervención humana y con una alta precisión. Como se representa en la Figura \ref{fig:ml}, las técnicas de \gls{ml} pueden pertenecer a tres categorías diferentes en función del enfoque de la información y de los objetivos que se pretenden conseguir con su aplicación: \cite{mlcat} \cite{iageeks} \cite{mltlf}

\begin{itemize}
    \item Aprendizaje no supervisado: Se parte de datos sin etiquetar para el entrenamiento de los modelos. Es decir, se parte del conocimiento de unos datos de entrada, pero no de la salida, por lo que el objetivo de los modelos no supervisados se basa en explorar posibles patrones en los datos mediante clustering.
    \item Aprendizaje supervisado: Se manejan conjuntos de datos con un conocimiento de las posibles salidas, las cuales se proporcionan en forma de etiquetas o de valores numéricos. Las técnicas de aprendizaje supervisado buscan un función óptima que consiga, dadas unas determinadas variables o características de entrada, predecir la salida con cierta precisión y son aplicables a dos tipos de problemas: de regresión, si a partir de las variables de entrada se desea predecir un valor numérico continuo a la salida, o de clasificación, si se predefinen diferentes clases y se asignan las variables de entrada a una de ellas. 
    \item Aprendizaje por refuerzo: Las técnicas englobadas en este tipo aprenden por prueba y error, por lo que su entrenamiento a lo largo del tiempo y la experiencia permiten optimizar los resultados sin la necesidad de disponer de un gran volumen de datos.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{img/teoria/ml.jpeg}
    \caption{Categorías de \acrshort{ml} \cite{metal}}
    \label{fig:ml}
\end{figure}

\vspace{3mm}

Una vez expuestas las diferentes categorías donde se engloban las técnicas de \gls{ml}, es preciso indicar que se va a centrar el estudio en el aprendizaje supervisado y, en particular, en la resolución de problemas de clasificación binaria. El motivo principal viene dado porque el objetivo que se persigue con la realización de este \gls{tfm} se basa en el desarrollo de modelos que permitan detectar y predecir posibles errores que se pueden producir en una \gls{sg} durante el proceso de distribución energética. 

\vspace{3mm}

Como se detallará más adelante en la Sección \ref{sec:cambiosden2ne}, se creará un conjunto de datos etiquetados en función de la existencia de error o no para entrenar los modelos. Teniendo en cuenta esto, a continuación, se incluyen dos Secciones (ver Secciones \ref{sec:mlsvm} y \ref{sec:mlrf}) para presentar el funcionamiento de las dos técnicas de \gls{ml} que se desarrollarán posteriormente.


\subsubsection{Support Vector Machines (\acrshort{svm})}
\label{sec:mlsvm}

El algoritmo de Máquinas de Vector Soporte (del inglés \gls{svm}) es una técnica que se emplea tanto en problemas de regresión como de clasificación. Se basa en el concepto del \textit{Maximal Margin Classifier}, puesto que tiene el fin principal de encontrar el hiperplano óptimo que consiga una máxima separación entre dos clases diferentes dentro de un espacio de características. Esta separación se define como margen y se mide como la distancia perpendicular que existe desde el hiperplano hacia los vectores de soporte, que son las muestras más cercanas a la frontera de separación de clases. En la Figura \ref{fig:svm} se representa el cálculo del mejor hiperplano entre dos clases de datos. \cite{svmciencia}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{img/teoria/svm.png}
    \caption{Cuantificación del hiperplano óptimo entre dos clases en el \acrshort{svm} \cite{svmciencia}}
    \label{fig:svm}
\end{figure}

\vspace{3mm}

Por ello, se puede expresar que los vectores de soporte son los puntos más críticos del plano para asignar una de las clases predefinidas a los nuevos vectores de datos a la entrada. Este proceso de clasificación viene dado por la siguiente expresión: \cite{svmmedium} 

\[ y_i(\mathbf{w} \cdot \mathbf{x_i} + b) \geq M\]

    Donde:
\begin{itemize}
    \renewcommand{\labelitemi}{}
    \item \(y_i\) es la etiqueta de la clase. 
    \item \(x_i\) es el vector de características.
    \item \(w\) es el vector de pesos o coeficientes del hiperplano.
    \item \(b\) es el sesgo ajustado en el modelo.
    \item \(M\) es el ancho definido para el margen.
\end{itemize}

\vspace{3mm}

No obstante, en la práctica es complicado determinar un hiperplano si no se trata de un problema ideal y linearmente separable. En la Figura \ref{fig:svmerror} se representa la opción alternativa que se lleva a cabo en la mayoría de casos, basada en obtener un margen máximo, permitiendo cierto grado de error. Esto tiene como consecuencia que existan una serie de vectores clasificados de forma errónea en el plano. \cite{svmmedium} \cite{matlab}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{img/teoria/svm2.png}
    \caption{Representación de vectores clasificados erróneamente dentro de los márgenes establecidos en el \acrshort{svm} \cite{svmmedium}}
    \label{fig:svmerror}
\end{figure}

\vspace{3mm}

En este proceso se incluyen las variables de holgura (\textit{slack variables}) para determinar si un vector está correctamente clasificado (ξi = 0) o si se encuentra en una zona errónea del margen (0<ξi<1) o del hiperplano (ξi>1).

\[ \xi_i = \max(0, 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b))\]


%Estas slack variables se introducen para permitir que las observaciones estén dentro del margen o incluso clasificadas incorrectamente, pero penalizadas por una función de pérdida, el valor de ξi representa la distancia a la margen o el error cometido. La suma de estas slack variables se incluye en el objetivo de la función de pérdida del SVM para minimizar el error de clasificación y maximizar el margen, y el parámetro de regularización C controla el equilibrio entre estos dos objetivos.






% SVM formula el problema de encontrar el hiperplano óptimo como un problema de optimización convexa, donde se minimiza la clasificación incorrecta y se maximiza el margen. Además, SVM utiliza el "Kernel Trick" para manejar problemas de clasificación no lineales, mapeando los datos a un espacio de características de mayor dimensionalidad donde es más probable que los datos sean linealmente separables. Finalmente, SVM incorpora un parámetro de regularización (C) que controla el equilibrio entre maximizar el margen y minimizar el error de clasificación en el conjunto de datos de entrenamiento, lo que permite un ajuste adecuado del modelo.

% Este espacio en el caso simplificado es de dos dimensiones, pero puede llegar a ser de k dimensiones en función de la linearidad de las características.




%Si bien la demostración matemática queda fuera del objetivo de esta introducción, es importante mencionar que el proceso incluye un hiperparámetro llamado  C.  Ccontrola el número y severidad de las violaciones del margen (y del hiperplano) que se toleran en el proceso de ajuste. Si  C=∞, no se permite ninguna violación del margen y por lo tanto, el resultado es equivalente al Maximal Margin Classifier (teniendo en cuenta que esta solución solo es posible si las clases son perfectamente separables). Cuando más se aproxima  Ca cero, menos se penalizan los errores y más observaciones pueden estar en el lado incorrecto del margen o incluso del hiperplano.  Ces a fin de cuentas el hiperparámetro encargado de controlar el balance entre bias y varianza del modelo. En la práctica, su valor óptimo se identifica mediante validación cruzada.El proceso de optimización tiene la peculiaridad de que solo las observaciones que se encuentran justo en el margen o que lo violan influyen sobre el hiperplano. A estas observaciones se les conoce como vectores soporte y son las que definen el clasificador obtenido. Esta es la razón por la que el parámetro  C controla el balance entre bias y varianza. Cuando el valor de  Ces pequeño, el margen es más ancho, y más observaciones violan el margen, convirtiéndose en vectores soporte. El hiperplano está, por lo tanto, sustentado por más observaciones, lo que aumenta el bias pero reduce la varianza. Cuando mayor es el valor de  C , menor el margen, menos observaciones son vectores soporte y el clasificador resultante tiene menor bias pero mayor varianza.Otra propiedad importante que deriva de que el hiperplano dependa únicamente de una pequeña proporción de observaciones (vectores soporte), es su robustez frente a observaciones muy alejadas del hiperplano. Esto hace al método de clasificación vector soporte distinto a otros métodos tales como Linear Discrimiant Analysis (LDA), donde la regla de clasificación depende de la media de todas las observaciones.









\subsubsection{Random Forest (\acrshort{rf})}
\label{sec:mlrf}




\subsection{Deep Learning (\acrshort{dl})}
\label{sec:dl}

El campo del aprendizaje profundo (\acrfull{dl}) se enfoca principalmente en la creación de redes neuronales que imiten el comportamiento y la estructura lógica que tiene el cerebro humano para buscar patrones en los datos. A diferencia de las técnicas de \gls{ml}, el \gls{dl} se vuelve más eficiente en el análisis de grandes volúmenes de datos, puesto que el \gls{ml} presenta ciertas limitaciones en este aspecto. \cite{metal} 

\vspace{3mm}

De la misma forma, el \gls{dl} presenta un mejor funcionamiento en la identificación de patrones cuando se manejan características complejas, aportando resultados con una mayor precisión que el \gls{ml}. Es por ello que las técnicas de \gls{dl} cobran una gran importancia en entornos donde los datos no son estructurados, como se produce en el caso de las imágenes, texto y audio. En este caso se pueden introducir en aplicaciones de reconocimiento de voz, procesado de lenguaje natural (\gls{nlp}) o visión artificial para la detección de objetos y clasificación de imágenes. \cite{iageeks}

\vspace{3mm}

Por otro lado, a nivel estructural las diferencias entre las técnicas de \gls{ml} y las de \gls{dl} radican principalmente en el tratamiento de las características o variables de los datos. En el caso de emplear \gls{ml}, si se manejan conjuntos con un gran número de características, sería necesario introducir un paso previo de diseño y selección de las más relevantes. Esto, como se puede apreciar en la Figura \ref{fig:features}, no ocurre en el desarrollo de un modelo de \gls{dl}, ya que el tratamiento se produce internamente dentro de la red reuronal. \cite{valohai}

\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{img/teoria/mlvsdl.png}
    \caption{Diferencias estructurales entre el \acrshort{ml} y el \acrshort{dl} \cite{valohai}}
    \label{fig:features}
\end{figure}

\vspace{3mm}

Una vez definida la rama de aprendizaje profundo, además de presentar las diferencias que presenta respecto a la de aprendizaje automático, se introduce una Sección dedicada a la exposición del modelo de \gls{dl} que se desarrollará como motivo del objetivo de este \gls{tfm} (ver Sección \ref{sec:dlann}). 

\subsubsection{Red Neuronal Artificial (\acrshort{ann})}
\label{sec:dlann}

Las redes neuronales artificiales (\gls{ann})




% de procesamiento de información

% Se modelan múltiples capas de neuronas para procesar los datos de entrada y aprender a través de un proceso iterativo de ajuste de los pesos de las conexiones entre neuronas.

% Además, tiene la capacidad de identificar patrones y características más complejas en los datos, lo que puede llevar a mejores resultados en aplicaciones como el reconocimiento de voz, la visión por computadora y el procesamiento del lenguaje natural.




\vspace{3mm}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{img/teoria/ann.jpg}
    \caption{Arquitectura de una \acrshort{ann} \cite{ann}}
    \label{fig:features}
\end{figure}

\vspace{3mm}







%hablar sobre ml
%relacionar ml con sg
\cite{impact}


%aqui hablar de ml para eficiencia de consumo


%ver figura pag6 cite stab -> esquema acciones desarrollo







%cite stab (para que se usan los algoritmos)

% Los algoritmos de predicción se utilizan para estimar la oferta y la demanda.
% de la red (Wang et al., 2018). En Singh y Yassine (2018), el
% Se abordó el análisis de datos de medidores inteligentes y se propusieron tres principales
% Aplicaciones: análisis de carga, previsión de carga y gestión de carga. El
% Las técnicas clave para estas aplicaciones incluyen series temporales, agrupación de datos, reducción de dimensionalidad, clasificación de datos, detección de valores atípicos,
% matriz de bajo rango y aprendizaje en línea. En Kezunovic et al. (2013), un
% Se propuso un modelo de minería de datos inteligente que se puede utilizar para analizar,
% predecir y visualizar patrones de consumo de series temporales de energía. A
% La parte clave de este modelo fue el uso de "minería de patrones frecuentes"; frecuente
% Los patrones son conjuntos de elementos que aparecen en un conjunto de datos con una frecuencia igual.
% o más que un umbral especificado por el usuario. La minería de patrones frecuente es una
% Herramienta de minería de datos esencial para procesar y analizar big data.



